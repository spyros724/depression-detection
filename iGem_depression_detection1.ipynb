{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyTQn7mk-Znt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D\n",
        "\n",
        "# Load the dataset into a pandas dataframe\n",
        "df = pd.read_csv('your_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the data by converting the text column to a sequence of integers, as well as splitting the dataset into training and testing sets:"
      ],
      "metadata": {
        "id": "C6Dc-af1-1M3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the text column to a sequence of integers\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "X = tokenizer.texts_to_sequences(df['text'])\n",
        "\n",
        "# Pad the sequences to ensure they are all the same length\n",
        "maxlen = 100\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(X, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "y = np.array(df['depression'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "lqN710d__Biy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the CNN model and training it on the training set:"
      ],
      "metadata": {
        "id": "WsBULhkC_MxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(maxlen, 1)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train.reshape(X_train.shape[0], maxlen, 1), y_train, epochs=10, batch_size=32, validation_split=0.1)\n"
      ],
      "metadata": {
        "id": "EH8-dysd_L4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the model on the testing set and print out the accuracy:"
      ],
      "metadata": {
        "id": "A1HsWtyh_aFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the testing set\n",
        "loss, accuracy = model.evaluate(X_test.reshape(X_test.shape[0], maxlen, 1), y_test)\n",
        "\n",
        "# Print out the accuracy\n",
        "print('Test accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "ndRJLORM_gH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you can use the trained model to make predictions on new data:"
      ],
      "metadata": {
        "id": "RJ837B2P_owM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on new data\n",
        "new_data = ['this is a sample text', 'this is another sample text']\n",
        "new_data = tokenizer.texts_to_sequences(new_data)\n",
        "new_data = tf.keras.preprocessing.sequence.pad_sequences(new_data, padding='post', maxlen=maxlen)\n",
        "predictions = model.predict(new_data.reshape(new_data.shape[0], maxlen, 1))\n",
        "\n",
        "# Print out the predictions\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "4jBoZKV3_qIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the weights from the trained model\n",
        "weights = model.get_weights()\n",
        "\n",
        "# Print the shape of each weight array\n",
        "for w in weights:\n",
        "    print(w.shape)\n"
      ],
      "metadata": {
        "id": "oQmAg1ofBR7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a function that uses these weights to classify new text inputs, you can define a new function that takes in a sequence of integers (representing the text) and applies the weights to make a prediction. Here's an example:"
      ],
      "metadata": {
        "id": "CjZK3X8_BlQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_text(text, weights):\n",
        "    # Convert the text to a sequence of integers\n",
        "    seq = tokenizer.texts_to_sequences([text])[0]\n",
        "    \n",
        "    # Pad the sequence to ensure it has the same length as the input to the CNN model\n",
        "    seq = tf.keras.preprocessing.sequence.pad_sequences([seq], padding='post', maxlen=maxlen)\n",
        "    \n",
        "    # Apply the weights to the sequence using the CNN model architecture\n",
        "    x = np.array(seq).reshape(1, maxlen, 1)\n",
        "    for i in range(len(weights)):\n",
        "        x = np.dot(x, weights[i])\n",
        "        if i < len(weights) - 1:\n",
        "            x = np.maximum(x, 0)\n",
        "    \n",
        "    # Return the predicted class (0 or 1)\n",
        "    return int(round(x[0][0]))\n"
      ],
      "metadata": {
        "id": "qb9qMknPBg4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes in a text input (a string), as well as the weights extracted from your trained CNN model. It first converts the text to a sequence of integers using the same tokenizer used to preprocess the original data. It then pads the sequence to ensure it has the same length as the input to the CNN model, and applies the weights to the sequence using the same architecture as the CNN model. Finally, it returns the predicted class as an integer (either 0 or 1).\n",
        "\n",
        "Here's an example of how to use this function:"
      ],
      "metadata": {
        "id": "-aRG1zesBpjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the weights from the trained model\n",
        "weights = model.get_weights()\n",
        "\n",
        "# Test the function on a new text input\n",
        "text = 'I am feeling really down today'\n",
        "class_prediction = classify_text(text, weights)\n",
        "print('Predicted class:', class_prediction)\n"
      ],
      "metadata": {
        "id": "NQD1jtqMBov6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}